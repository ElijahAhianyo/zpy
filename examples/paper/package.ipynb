{
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Case Study: Package Detection with Synthetic Data\n",
    "\n",
    "<a href=\"https://www.zumolabs.ai/?utm_source=github.com&utm_medium=referral&utm_campaign=zpy\"><img src=\"https://github.com/ZumoLabs/zpy/raw/main/docs/assets/zl_tile_logo.png\" width=\"100px\"/></a>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://discord.gg/nXvXweHtG8\"><img alt=\"Discord\" title=\"Discord\" src=\"https://img.shields.io/badge/-ZPY Devs-grey?style=for-the-badge&logo=discord&logoColor=white\"/></a>\n",
    "  <a href=\"https://twitter.com/ZumoLabs\"><img alt=\"Twitter\" title=\"Twitter\" src=\"https://img.shields.io/badge/-@ZumoLabs-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white\"/></a>\n",
    "  <a href=\"https://www.youtube.com/channel/UCcU2Z8ArljfDzfq7SOz-ytQ\"><img alt=\"Youtube\" title=\"Youtube\" src=\"https://img.shields.io/badge/-ZumoLabs-red?style=for-the-badge&logo=youtube&logoColor=white\"/></a>\n",
    "</p>\n",
    "\n",
    "<a href=\"Example synthetic package images.\"><img src=\"https://github.com/ZumoLabs/zpy/raw/main/docs/assets/package_sim_boxes.png\" width=\"600px\"/></a>\n",
    "\n",
    "<a href=\"Results from model trained on package sim dataset.\"><img src=\"https://github.com/ZumoLabs/zpy/raw/main/docs/assets/package_sim_results.png\" width=\"600px\"/></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<!-- ![Example synthetic package images.](https://github.com/ZumoLabs/zpy/raw/main/docs/assets/package_sim_boxes.png)\n",
    "\n",
    "![Results from model trained on package sim dataset.](https://github.com/ZumoLabs/zpy/raw/main/docs/assets/package_sim_results.png) -->\n",
    "\n",
    "In this example, we train a detection model which predicts the bounding boxes for cardboard packages and parcels in images. We fine tune a pre-trained model on small synthetic datasets with different types of domain randomization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation\n",
    "\n",
    "## Running Locally with Docker\n",
    "\n",
    "You can connect this notebook to the provided Docker container to train locally with your GPU.  To build the docker image:\n",
    "\n",
    "```\n",
    "docker build \\\n",
    "     -t \"zumolabs/package\" \\\n",
    "     -f Dockerfile.gpu .\n",
    "```\n",
    "\n",
    "To run the docker image:\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "    --gpus all \\\n",
    "    -p 8888:8888 \\\n",
    "    -p 6006:6006 \\\n",
    "    -v /tmp:/tmp \\\n",
    "    -v /home/tren/data:/data \\\n",
    "    zumolabs/package\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# What GPU is currently connected?\n",
    "!nvidia-smi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log into Zumo Labs\n",
    "\n",
    "To generate data on the Zumo Labs cloud you will first need to [create an account](https://www.zumolabs.ai/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import zpy\n",
    "from zpy import client\n",
    "\n",
    "# This is your temporary authtoken. It can be found by visiting:\n",
    "#     https://app.zumolabs.ai/settings/auth-token\n",
    "#\n",
    "# The auth token will expire when you log out of the web app\n",
    "auth_token = \"...\"\n",
    "\n",
    "zpy.client.init(project_uuid='...', auth_token=auth_token)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Training (Synthetic) Data\n",
    "\n",
    "We will create 4 separate synthetic datasets for training, each with different types of *Domain Randomization*: a technique commonly used in synthetic data to increase the variance of a dataset distribution.\n",
    "\n",
    "- `package_sim_dr_light` - Synthetic dataset of 512 images. Domain randomization is applied to lighting only. The position of a sun light object in the scene, as well as the intensity of the light, is randomized within a range for every image\n",
    "- `package_sim_dr_mats` - Synthetic dataset of 512 images. Domain randomization is applied to materials only. The material of each individial package is created in each image, starting with a randomly chosen texture from a library of thousands of textures scraped from the internet. Several properties of the material, such as specular, metallic, and roughness are then jittered within a broad range.\n",
    "- `package_sim_dr_bg` - Synthetic dataset of 512 images. Domain randomization is applied to background only. Each image is rendered with a different HDRI, which is chosen from a library of hundreds of HDRIs scraped from the internet.\n",
    "- `package_sim_dr_all` - Synthetic dataset of 512 images. Domain randomization is applied to lighting, background, and materials.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_config = zpy.client.DatasetConfig('package_v3')\n",
    "_config.set('run\\.domain_randomize_lights', True)\n",
    "_config.set('run\\.domain_randomize_materials', False)\n",
    "_config.set('run\\.domain_randomize_background', False)\n",
    "zpy.client.generate('package_sim_DR_lights', _config, num_datapoints=256, materialize=True)\n",
    "\n",
    "_config = zpy.client.DatasetConfig('package_v3')\n",
    "_config.set('run\\.domain_randomize_lights', False)\n",
    "_config.set('run\\.domain_randomize_materials', True)\n",
    "_config.set('run\\.domain_randomize_background', False)\n",
    "zpy.client.generate('package_sim_DR_mats', _config, num_datapoints=256, materialize=True)\n",
    "\n",
    "_config = zpy.client.DatasetConfig('package_v3')\n",
    "_config.set('run\\.domain_randomize_lights', False)\n",
    "_config.set('run\\.domain_randomize_materials', False)\n",
    "_config.set('run\\.domain_randomize_background', True)\n",
    "zpy.client.generate('package_sim_DR_bg', _config, num_datapoints=256, materialize=True)\n",
    "\n",
    "_config = zpy.client.DatasetConfig('package_v3')\n",
    "_config.set('run\\.domain_randomize_lights', True)\n",
    "_config.set('run\\.domain_randomize_materials', True)\n",
    "_config.set('run\\.domain_randomize_background', True)\n",
    "zpy.client.generate('package_sim_DR_all', _config, num_datapoints=256, materialize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Lets make a small helper object to keep track of the location of\n",
    "# the images and the annotation files for each dataset\n",
    "Dataset = namedtuple('Dataset', ['name', 'image_directory_path', 'annotation_file_path'])\n",
    "\n",
    "package_sim_DR_lights = Dataset(\n",
    "    name='package_sim_DR_lights',\n",
    "    image_directory_path='/data/package_ablation/package-sim-dr-lights',\n",
    "    annotation_file_path='/data/package_ablation/package-sim-dr-lights/_annotations.coco.json',\n",
    ")\n",
    "\n",
    "package_sim_DR_mats = Dataset(\n",
    "    name='package_sim_DR_mats',\n",
    "    image_directory_path='/data/package_ablation/package-sim-dr-mats',\n",
    "    annotation_file_path='/data/package_ablation/package-sim-dr-mats/_annotations.coco.json',\n",
    ")\n",
    "\n",
    "package_sim_DR_bg = Dataset(\n",
    "    name='package_sim_DR_bg',\n",
    "    image_directory_path='/data/package_ablation/package-sim-dr-bg',\n",
    "    annotation_file_path='/data/package_ablation/package-sim-dr-bg/_annotations.coco.json',\n",
    ")\n",
    "\n",
    "package_sim_DR_all = Dataset(\n",
    "    name='package_sim_DR_all',\n",
    "    image_directory_path='/data/package_ablation/package-sim-dr-all',\n",
    "    annotation_file_path='/data/package_ablation/package-sim-dr-all/_annotations.coco.json',\n",
    ")\n",
    "\n",
    "package_real_test = Dataset(\n",
    "    name='package_real_test',\n",
    "    image_directory_path='/data/package_ablation/package-real-test',\n",
    "    annotation_file_path='/data/package_ablation/package-real-test/_annotations.coco.json',\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize Data\n",
    "\n",
    "Look through some sample images for each synthetic dataset with a different kind of domain randomization.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import zpy.viz\n",
    "from pathlib import Path\n",
    "\n",
    "def visualize_dataset(dataset : Dataset, search_string : str = '**/*.rgb.png'):\n",
    "    \"\"\" Plot some sample images of a dataset\"\"\"\n",
    "    opened_images = [zpy.image.open_image(i) for i in Path(dataset.image_directory_path).glob(search_string)]\n",
    "    print(f'Found {len(opened_images)} images for {dataset.name}')\n",
    "    zpy.viz.image_grid_plot(images=opened_images, output_path='/tmp', show=True)\n",
    "\n",
    "# Synthetic data for training\n",
    "print('\\n\\n Synthetic Dataset with Material Domain Randomization \\n\\n')\n",
    "visualize_dataset(package_sim_DR_mats)\n",
    "print('\\n\\n Synthetic Dataset with Background Domain Randomization \\n\\n')\n",
    "visualize_dataset(package_sim_DR_bg)\n",
    "print('\\n\\n Synthetic Dataset with Lighting Domain Randomization \\n\\n')\n",
    "visualize_dataset(package_sim_DR_lights)\n",
    "print('\\n\\n Synthetic Dataset with All Domain Randomization \\n\\n')\n",
    "visualize_dataset(package_sim_DR_all)\n",
    "\n",
    "\n",
    "# Real data for testing\n",
    "print('\\n\\n Real Images \\n\\n')\n",
    "visualize_dataset(package_real_test, '**/*.jpg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train\n",
    "\n",
    "Training code (`train_code.py`) fine-tunes on synthetic data and then evaluates on the real data, outputting prediction images.\n",
    "\n",
    "The pre-trained models come from the [Detectron2 Model Zoo](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md), more info on the models:\n",
    "\n",
    "- Includes basic data augmentation: scale jittering and horizontal flipping.\n",
    "\n",
    "- 3 different backbone combinations:\n",
    "    - **FPN**: Use a ResNet+FPN backbone with standard conv and FC heads for mask and box prediction, respectively. It obtains the best speed/accuracy tradeoff, but the other two are still useful for research.\n",
    "\n",
    "    - **C4**: Use a ResNet conv4 backbone with conv5 head. The original baseline in the Faster R-CNN paper.\n",
    "\n",
    "    - **DC5** (Dilated-C5): Use a ResNet conv5 backbone with dilations in conv5, and standard conv and FC heads for mask and box prediction, respectively. This is used by the Deformable ConvNet paper.\n",
    "\n",
    "- Trained with the 3x schedule (~37 COCO epochs). Although 1x models are heavily under-trained, we provide some ResNet-50 models with the 1x (~12 COCO epochs) training schedule for comparison when doing quick research iteration.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Does the GPU work?\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "# Grid search over iterations, learning rate, models, training dataset\n",
    "train_dataset = [\n",
    "    package_sim_DR_lights,\n",
    "    package_sim_DR_mats,\n",
    "    package_sim_DR_bg,\n",
    "    package_sim_DR_all,\n",
    "]\n",
    "models = [\n",
    "    # 'faster_rcnn_R_50_C4_1x',\n",
    "    'faster_rcnn_R_50_C4_3x',\n",
    "    # 'faster_rcnn_R_101_C4_3x',\n",
    "    # 'faster_rcnn_R_50_DC5_1x',\n",
    "    'faster_rcnn_R_50_DC5_3x',\n",
    "    # 'faster_rcnn_R_101_DC5_3x',\n",
    "    'faster_rcnn_R_50_FPN_1x',\n",
    "    'faster_rcnn_R_50_FPN_3x',\n",
    "    'faster_rcnn_R_101_FPN_3x',\n",
    "]\n",
    "iterations = [2, 8, 32, 64, 128, 256, 512, 1024]\n",
    "learning_rate = [0.001, 0.0001, 0.00001]\n",
    "grid_search = list(itertools.product(train_dataset, iterations, learning_rate, models))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove any existing training logs\n",
    "!rm -rf /tmp/package_logs3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from train_code import train\n",
    "from pathlib import Path\n",
    "\n",
    "# random.shuffle(grid_search)\n",
    "for train_dataset, iters, lr, model in grid_search:   \n",
    "    train(\n",
    "        # Directory for output of training logs and prediction images\n",
    "        output_dir = Path('/tmp/package_logs3') / train_dataset.name / model / str(iters) / str(lr),\n",
    "        model=model, \n",
    "        test_thresh=0.9, # How good do predictions have to be to be evaluated? 0.7 = 70% confidence\n",
    "        iters=iters, # Total iterations = dataset size / batch size \n",
    "        lr=lr,\n",
    "        batch_size=2, # Depends on your GPU memory and the size of the model and images\n",
    "        class_dict={0: 'box'}, # Cardboard package box detection task\n",
    "        # Fine-tune on synthetic data\n",
    "        train_name = train_dataset.name,\n",
    "        train_image_dir_path = train_dataset.image_directory_path,\n",
    "        train_annotation_file_path = train_dataset.annotation_file_path,\n",
    "        # Test on real data\n",
    "        test_name = package_real_test.name,\n",
    "        test_image_dir_path = package_real_test.image_directory_path,\n",
    "        test_annotation_file_path = package_real_test.annotation_file_path,\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Logs\n",
    "\n",
    "You can view the training logs through tensorboard by navigating to [http://127.0.0.1:6006](http://127.0.0.1:6006)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/package_logs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predictions\n",
    "\n",
    "Visualize the top performing runs for each type of domain randomization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import zpy.viz\n",
    "from pathlib import Path\n",
    "\n",
    "def visualize_predictions(prediction_image_dir_path : str):\n",
    "    \"\"\" Plot some sample predictions\"\"\"\n",
    "    opened_images = [zpy.image.open_image(i) for i in Path(prediction_image_dir_path).glob('**/*.jpg')]\n",
    "    zpy.viz.image_grid_plot(images=opened_images, output_path='/tmp', show=True)\n",
    "\n",
    "print('\\n\\n Material Domain Randomization \\n\\n')\n",
    "visualize_predictions('/tmp/package_logs/package_sim_DR_mats/faster_rcnn_R_50_C4_1x/500/0.001/predictions')\n",
    "print('\\n\\n Background Domain Randomization \\n\\n')\n",
    "visualize_predictions('/tmp/package_logs/package_sim_DR_bg/faster_rcnn_R_50_C4_1x/128/0.001/predictions')\n",
    "print('\\n\\n Lighting Domain Randomization \\n\\n')\n",
    "visualize_predictions('/tmp/package_logs/package_sim_DR_lights/faster_rcnn_R_50_C4_1x/256/0.001/predictions')\n",
    "print('\\n\\n All Domain Randomization \\n\\n')\n",
    "visualize_predictions('/tmp/package_logs/package_sim_DR_all/faster_rcnn_R_50_C4_1x/256/0.001/predictions')\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}
\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage[square,numbers]{natbib}
\usepackage{doi}

\usepackage{xcolor}
\usepackage{listings}		% code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{ZPY: Open Source Synthetic Data for Computer Vision}

\author{
	{\hspace{1mm}Hugo Ponte}\thanks{correspondence author} \\ 
	Zumo Labs \\
	\texttt{hugo@zumolabs.ai} \\
	\And
	{\hspace{1mm}Norman Ponte} \\
	Zumo Labs \\
	\texttt{norman@zumolabs.ai} \\
	\And
	{\hspace{1mm}Sammie Crowder} \\
	Zumo Labs \\
	\texttt{sammie@zumolabs.ai} \\
	\And
	{\hspace{1mm}Kory Stiger} \\
	Zumo Labs \\
	\texttt{kory@zumolabs.ai} \\
	\And
	{\hspace{1mm}Steven Pecht} \\
	Zumo Labs \\
	\texttt{steven@zumolabs.ai} \\
	\And
	{\hspace{1mm}Michael Stewart} \\
	Zumo Labs \\
	\texttt{michael@zumolabs.ai} \\
	\And
	{\hspace{1mm}Elena Ponte} \\
	Zumo Labs \\
	\texttt{elena@zumolabs.ai} \\
}

% Uncomment to remove the date
\date{}

\renewcommand{\headeright}{Technical Report}
\renewcommand{\undertitle}{Technical Report}

% PDF metadata
\hypersetup{
pdftitle={ZPY: Open Source Synthetic Data for Computer Vision},
pdfsubject={cs.CV},
pdfauthor={Hugo Ponte, Norman Ponte, Sammie Crowder, Kory Stiger, Steven Pecht, Elena Ponte},
pdfkeywords={Computer Vision, Synthetic Data, Machine Learning, Open Source, Python, Blender},
}

\begin{document}
\maketitle

\begin{abstract}
Synthetic data presents a unique solution to the huge data requirements of computer vision with deep learning. In this work, we present zpy \footnote{All code is available on GitHub at \url{http://github.com/ZumoLabs/zpy}}, an open source framework for creating synthetic data. Built on top of Blender, and designed with modularity and readability in mind.
\end{abstract}

\keywords{Computer Vision \and Synthetic Data \and Machine Learning \and Open Source \and Python \and Blender}

% https://www.overleaf.com/learn/latex/Inserting_Images
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\textwidth]{cover.png}
	\caption{Synthetic images of a raspberri pi created with zpy: (A) color image, (B) segmentation image, (C) depth image. These images are used to train a deep learning model, which predicts bounding boxes on key components as seen in the (D) prediction image}
	\label{fig:fig1}
\end{figure}

\section{Introduction}
\label{sec:introduction}

Open source machine learning frameworks (references)

Deep learning has exploded in popularity due to large open source frameworks such as tensorflow and pytorch
What is synthetic data (references)

Challenges with synthetic data

Sim2Real Gap and domain randomization

Black box nature of ML

\section{Background}
\label{sec:background}

\subsection{3D}
\label{sec:3d}

3D, short for the three dimensions of space we live in, is a catch-all term used to describe the varied technologies used to create virtual worlds. The technology stack for 3D can be roughly split into two broad categories: \emph{asset creation} and \emph{asset scripting}. Asset creation \ref{sec:assetcreation} is the process of creating assets: virtual objects, scenes, and materials. Asset scripting \ref{sec:assetscripting} is the process of manipulating those assets and their interactions over the fourth dimension of time. Decades of progress have resulted in sophisticated software tools that make 3D workflows easier and more automated, but a significant amount of human expertise and artistic talent is still required.
 
\subsubsection{Asset Creation}
\label{sec:assetcreation}

Assets are digital representations of a 3D object. One type of asset is a mesh: a connected graph of 3D points, also called vertices, which define the surface of an object. Vertices are interconnected by edges, and a closed loop of vertices creates a polygon known as a face. In the engineering and manufacturing world, meshes are created using computer aided design (CAD) software such as AutoCAD \citep{autocad}, Solidworks \citep{solidworks}, Onshape \citep{onshape}, and Rhino \citep{rhino}. In the entertainment industry, meshes are created using modeling software such as Maya \citep{maya}, 3DSMax \citep{3dsmax}, and Cinema4D \citep{cinema4d}.

Whereas a mesh describes the shape and form of an object, a material asset describes the texture and appearance of a virtual object. A material may define rules for the reflectivity, specularity, and metallic-ness of the object as a function of lighting conditions. Materials are used by shader programs to calculate the exact pixel values to render for each face of a mesh polygon. Modeling software will usually come packaged with tools that allow the creation and configuration of materials.

Finally, asset creation also encompasses the process of scene composition. Assets can be organized into scenes, which may contain other unique virtual objects such as simulated lights and cameras. Deciding where to place assets, especially lights, is still almost entirely done by hand. Automatic scene composition remains an outstanding challenge in the 3D technology stack.


\subsubsection{Asset Scripting}
\label{sec:assetscripting}

The fourth perceivable dimension of our reality is time. Asset scripting is the process of defining the behaviors of assets within scenes over time. One type of asset scripting is called animation, which consists of creating a set of sequential mesh deformations to create the illusion of natural movement. Animation is a tedious manual task since every frame must be defined: expert animators spend decades honing their digital puppeteering skills. Specialized software is often used to automate this task as much as possible, and technologies such as Motion Capture (MoCap) can be used to record the movement of real objects and play those movements back on virtual assets.

Game Engines are software tools which allow for asset scripting in a more structured and systematic way, mostly by providing software interfaces (e.g. code) to control the virtual world. Used extensively in the video game industry after which they were named, examples include Unity \citep{unity3d}, Unreal Engine \citep{unrealengine}, GoDot \citep{godot}, and Roblox \citep{roblox}. These game engines allow for rule-based spawning, animation, and complex interactions between assets in the virtual world. Programming within game engines is a separate skillset to modeling and animating and is usually done by separate engineers within an organization.

\subsubsection{Blender}
\label{sec:blender}

Blender is an open source 3D software tool originally released in 1994 \citep{blender}. It has grown steadily over the decades and has become one of the most popular 3D tools available, with a massive online community of users. Blender strength in its breadth: it provides simple tools for every part of the 3D workflow, rather than specializing in a narrow slice of the 3D workflow. Organizations such as game studios have traditionally preferred specialization, having separate teams of engineers using separate tools, such as Maya for modeling and Unreal Engine for scripting. However, the convenience of using a single tool, combined with the advantage of a single engineer being able to see a project start to finish, make a strong case for Blender as the ultimate winner in the 3D software tools race. By focusing on open source and community, Blender continues to gain adoption as more and more of the world’s new 3D developers choose to learn Blender over other more specialized tools.

In addition to encompassing the full breadth of the 3D workflow, Blender has the unique strength of using Python as the programming language of choice for asset scripting. Python has emerged as the lingua franca for modern deep learning, rising to the top in part due to the popularity of open source frameworks such as TensorFlow \citep{tensorflow}, PyTorch \citep{pytorch}, and Scikit-Learn \citep{scikit-learn}. If we wish for Machine Learning Engineers to perform asset scripting, they will be much more comfortable in Blender’s Python environment compared to the C# of Unity or the C++ of Unreal Engine.


\subsection{Synthetic Data}
\label{sec:syntheticdata}

The synthetic data workflow can be broadly divided into three categories: simulation design, generation, and tuning.

BlenderProc, Synthesis, LexSet

\section{Motivation}
\label{sec:motivation}

\subsection{Democratization of Data}

The datasets used by companies today are almost exclusively collected. Images and annotations are gradually saved as they are generated by users of a product. Collecting a dataset large enough to train a robust machine learning model can take years.

Only companies that have the scale and have set up the infrastructure to collect and store large datasets will have the datasets required to train models.

In order to compete, small or newly formed companies often resort to purchasing data from a third party supplier. This market for the selling and reselling of collected data presents an existential threat to privacy. Though some regulations have emerged, famously GDPR in Europe, these have yet to change the landscape of the market for collected data.

The high cost of data labeling makes it unavailable to those without huge resources

\subsection{Fairness and Bias}

The real world is biased and unfair.

ML systems trained on this data will reflect the same bias during inference.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{software3.png}
	\caption{Software 3.0: the developer transitions from writing explicit algorithms to creating and curating datasets which are used to train machine learning models.}
	\label{fig:fig2}
\end{figure}

\subsection{Software 3.0}
\label{sec:software3.0}

Data creation as a new paradigm for “programming”.

In the software world today, developers write explicit sets of rules (known as algorithms). These algorithms are then deployed into production systems, which consume input data and output actions.

In the software of tomorrow, developers will curate a dataset which will be used to train a deep learning model. This model will then be deployed into a production system, which will consume input data and output actions. This changes the workflow of developers from explicitly writing rules to instead creating the datasets which are then parsed to create algorithms.

\section{Project Features}
\label{sec:projectfeatures}

Given 

\subsection{Blender Addon}
\label{sec:blenderaddon}

As explained in section X, deep learning is a python-first discipline. If we wish to include deep learning practitioners in the 3D stack it is thus of critical importance that we provide a python interface for the 3D workflow.

\subsection{Cloud Backend}
\label{sec:cloudbackend}

Computing has traditionally relied on Moore’s Law to increase the power of individual computers. In the past decade the individual compute power of a single computer has not increased significantly, and instead the ability to coordinate a large number of individual computers on a single task has become the method for increasing computation. This type of parallel computing has been democratized through the availability of cloud computing platforms such as AWS, GCP, and Azure. However, these platforms remain difficult for the average developer to use effectively, and domain experts are usually required to take software running on a single computer and scale it across many computers in parallel.

Abstracting away the difficulties of the cloud workflow and providing an intuitive and convenient interface for parallelizing computation is thus important for the democratization of deep learning and synthetic data workflows. Much like platforms have emerged for the training and tuning workflows of deep learning, an opportunity exists to create platforms for the generation and tuning workflows of synthetic data.

\subsection{User Interfaces}
\label{sec:userinterfaces}

We provide three different interfaces to interact with our product: a Python API \ref{lst:api}, a CLI \ref{lst:cli}, and a graphical WebApp \ref{fig:webapp}. Power users want an API (application programming interface) and CLI (command line interface). Building a ramp for the bulk of the developer community requires a GUI

\begin{lstlisting}[language=Python,caption={Generating a dataset using the zpy python API.},label={lst:api}]
import zpy
zpy.generate()
\end{lstlisting}

\begin{lstlisting}[language=bash,caption={Generating a dataset using the zpy CLI},label={lst:cli}]
# First you will need to log in and set the project (image generation gets billed according to project)
zpy login $USERNAME 
zpy project set $PROJECT_UUID

# Generate the dataset
zpy create dataset "redbull cans and packs" can_v6 num_images 1000
\end{lstlisting}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{webapp.png}
	\caption{A visual interface for synthetic data creation via a WebApp.}
	\label{fig:webapp}
\end{figure}


\section{Design}
\label{sec:design}

Hidden complexity

When designing software systems, there is usually a tradeoff between flexibility and simplicity. Simplicity is the ability to perform a task with minimal amount of work and a limited understanding of the software package. Flexibility is the ability to support many different tasks and allow for customization.

Random hdri and textures use default random textures unless a specific path is given

Pythonic syntax

One of the core features of Python is the language’s human readable syntax. Python does not enforce function and variable type annotations, which makes it quicker to prototype code.

Functions in zpy are flexible in the arguments that they accept. The `zpy.opject.segment()` function call can accept an object directly of type `bpy.types.Object`, but it will also accept the unique string name of that object. 

Extensibility

Zpy modules are separated by dependencies

Zpy modules are independent of each other, a monolithic system is much harder to update and maintain


\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{workflow.png}
	\caption{The Synthetic Data Workflow allows for iteration of the dataset.}
	\label{fig:workflow}
\end{figure}

\section{Workflow}
\label{sec:workflow}

The full workflow for synthetic data can be reduced into four key steps: \emph{Design} \ref{sec:worflowdesign}, \emph{Generation} \ref{sec:generation}, \emph{Evaluation} \ref{sec:evaluation}, \emph{Iteration} \ref{sec:iteration}. The synthetic data workflow is similar to the workflow when using collected data, with the key exception that it allows for iteration on the dataset itself \ref{fig:workflow}.

\subsection{Design}
\label{sec:worflowdesign}

The first step in the syntehtic data workflow is to design and create the sim, short for simulation. A sim is a collection of assets controlled through a single script. The run() function in the script acts as the point of entry for any generation process. Relevant parameters are put as kwargs in the run() function, which allows configuration through gin-config, a python package for configuration of python libraries.

\subsection{Generation}
\label{sec:generation}

Datasets are rendered frame by frame. Separate rendering passes are required for segmentation and color images.

Datasets can be rendered locally directly inside the Blender GUI. This allows for easy debugging when developing a sim. Once it is working locally, a sim can be exported into a zip file which will contain all the asset dependencies required. This zip file can be uploaded to the cloud with any of the user interfaces described in \ref{sec:userinterfaces}.

Dataset are then generated in the cloud \ref{sec:cloudbackend}.

We provide workflows for packaging several smaller datasets into a single larger dataset, as well as sorting individual datapoints into test, train, and validation buckets.

\subsection{Evaluation}
\label{sec:evaluation}

Sweeping over dataset hyperparameters (much like sweeping over model architecture parameters)

Fine-tuning pre-trained models as a proxy for dataset performance

\subsection{Iteration}
\label{sec:iteration}

Least understood part of developing with synthetic data

The promise of AutoML for datasets


\section{Example}
\label{sec:example}

Package detection

\begin{itemize}
	\item Background material and texture.
	\item Lighting intensity and positioning.
	\item Object material and texture.
	\item Number of objects in view.
\end{itemize}

\subsection{Domain Randomization}
\label{sec:domainrandomization}

Effect of domain randomized lighting

Effect of domain randomized backgrounds

\subsection{Training Curicculum}
\label{sec:curicculum}

Pre-training w/ real and fine-tuning on synthetic

Training only on synthetic

Training on mixed synthetic and real

\section{Conclusion}
\label{sec:conclusion}

TODO

\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}
